"""
Dask-Compatible Advanced Feature Engineering
Adapts the advanced feature engineering modules to work with Dask DataFrames.
"""

import dask.dataframe as dd
import pandas as pd
import numpy as np
from typing import Dict, List
import logging

# Assuming the original feature engineering scripts are in the path
from temporal_invariant_features import TemporalInvariantExtractor
from tls_entropy_analyzer import TLSEntropyAnalyzer

logger = logging.getLogger(__name__)

def dask_temporal_feature_extractor(df: pd.DataFrame) -> pd.Series:
    """
    Wrapper function to apply TemporalInvariantExtractor to a Dask groupby.
    This function is applied to each group (a flow) in the Dask DataFrame.
    
    Args:
        df: A pandas DataFrame representing a single flow's packets.
        
    Returns:
        A pandas Series with the extracted temporal features.
    """
    # The input df is a group of packets. We can convert it to the expected
    # format for the extractor, which is a list of packet dictionaries.
    # This is more efficient than iterating with iterrows().
    packets = df.to_dict('records')
    flow = {'packets': packets}
    
    extractor = TemporalInvariantExtractor()
    features_df = extractor.extract_flow_invariants([flow])
    
    return features_df.iloc[0]

def dask_tls_feature_extractor(df: pd.DataFrame) -> pd.Series:
    """
    Wrapper function to apply TLSEntropyAnalyzer to a Dask groupby.
    
    Args:
        df: A pandas DataFrame representing a single flow's packets.
        
    Returns:
        A pandas Series with the extracted TLS features.
    """
    # This function makes assumptions about the input DataFrame's columns.
    # These columns would need to be generated by a packet parsing tool (e.g., tshark, scapy)
    # and be present in the initial packet-level CSV.
    
    # Assumed columns:
    # - 'is_handshake': boolean, True if the packet is part of a TLS handshake
    # - 'tls_version': string, e.g., 'TLSv1.3'
    # - 'cipher_suites': list of strings, from Client Hello
    # - 'key_exchange': string, from Server Hello
    # - 'certificate_size': int, size of certificate
    # - 'certificate_issuer': string, issuer of certificate
    # - 'certificate_validity_days': int, validity period
    # - 'extensions': list of strings, TLS extensions offered
    
    handshake_packets = df[df.get('is_handshake', pd.Series([False]*len(df)))].to_dict('records')
    
    # Aggregate TLS info from the flow's packets
    tls_flow_info = {
        'handshake_packets': handshake_packets,
        'cipher_suites': df['cipher_suites'].dropna().iloc[0] if 'cipher_suites' in df.columns and not df['cipher_suites'].dropna().empty else [],
        'certificates': df[df['certificate_size'] > 0].to_dict('records') if 'certificate_size' in df.columns else [],
        'extensions': df['extensions'].dropna().iloc[0] if 'extensions' in df.columns and not df['extensions'].dropna().empty else [],
        'tls_version': df['tls_version'].dropna().iloc[0] if 'tls_version' in df.columns and not df['tls_version'].dropna().empty else 'unknown',
        'key_exchange': df['key_exchange'].dropna().iloc[0] if 'key_exchange' in df.columns and not df['key_exchange'].dropna().empty else '',
    }

    analyzer = TLSEntropyAnalyzer()
    features_df = analyzer.extract_handshake_features([tls_flow_info])
    
    return features_df.iloc[0]


def apply_advanced_feature_engineering(ddf: dd.DataFrame) -> dd.DataFrame:
    """
    Applies advanced feature engineering to a Dask DataFrame of packets.
    
    Args:
        ddf: A Dask DataFrame where each row is a packet, with a 'Flow ID' column.
        
    Returns:
        A Dask DataFrame with one row per flow, containing the engineered features.
    """
    logger.info("Applying advanced feature engineering with Dask...")

    # If there is no Flow ID column, we cannot aggregate per-flow features.
    # In that case, simply return the original DataFrame and let the caller
    # decide how to proceed.
    if 'Flow ID' not in ddf.columns:
        logger.warning("No 'Flow ID' column found in input to advanced feature engineering; returning input DataFrame unchanged.")
        return ddf
    
    # --- Temporal Features ---
    logger.info("  - Extracting temporal features...")
    # Define metadata for temporal features
    sample_flow_temporal = {'packets': [{'timestamp': 0, 'size': 0, 'direction': 'f'}]}
    temp_extractor = TemporalInvariantExtractor()
    meta_temporal = temp_extractor.extract_flow_invariants([sample_flow_temporal]).iloc[[0]]  # Make it a DataFrame
    
    temporal_features = ddf.groupby('Flow ID').apply(
        dask_temporal_feature_extractor,
        meta=meta_temporal
    )
    
    # --- TLS Features ---
    logger.info("  - Extracting TLS features...")
    # Define metadata for TLS features
    sample_flow_tls = {'handshake_packets': []}
    tls_analyzer = TLSEntropyAnalyzer()
    meta_tls = tls_analyzer.extract_handshake_features([sample_flow_tls]).iloc[0]
    
    # Check if required columns for TLS analysis exist
    required_tls_cols = ['is_handshake', 'tls_version', 'cipher_suites', 'key_exchange', 'certificate_size']
    if all(col in ddf.columns for col in required_tls_cols):
        tls_features = ddf.groupby('Flow ID').apply(
            dask_tls_feature_extractor,
            meta=meta_tls
        )
        # Combine features
        combined_features = dd.concat([temporal_features, tls_features], axis=1)
        logger.info("✓ Combined temporal and TLS features.")
    else:
        logger.warning("  - Skipping TLS feature extraction. Required columns not found.")
        combined_features = temporal_features
        logger.info("✓ Using only temporal features.")
    
    return combined_features
