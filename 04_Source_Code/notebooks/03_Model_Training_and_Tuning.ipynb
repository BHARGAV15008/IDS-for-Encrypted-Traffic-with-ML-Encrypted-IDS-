{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P22 IDS - Model Training and Hyperparameter Tuning\n",
    "\n",
    "This notebook demonstrates advanced model training features.\n",
    "\n",
    "## What You'll Learn:\n",
    "1. Hyperparameter tuning with Optuna\n",
    "2. K-Fold cross-validation\n",
    "3. Model training with early stopping\n",
    "4. Visualize training history\n",
    "5. Save and load trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from services.modelTrainingService import ModelTrainingService\n",
    "from orchestrator import ServiceOrchestrator\n",
    "\n",
    "print(\"✓ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic training data for demonstration\n",
    "# Replace with your actual dataset\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic features (1000 samples, 20 features)\n",
    "n_samples = 1000\n",
    "n_features = 20\n",
    "n_classes = 5\n",
    "\n",
    "# Normal traffic (class 0)\n",
    "normal_features = np.random.normal(0.5, 0.1, (600, n_features))\n",
    "normal_labels = np.zeros(600, dtype=int)\n",
    "\n",
    "# Attack traffic (classes 1-4)\n",
    "attack_features = np.random.normal(0.7, 0.15, (400, n_features))\n",
    "attack_labels = np.random.randint(1, n_classes, 400)\n",
    "\n",
    "# Combine\n",
    "features = np.vstack([normal_features, attack_features])\n",
    "labels = np.hstack([normal_labels, attack_labels])\n",
    "\n",
    "# Shuffle\n",
    "indices = np.random.permutation(n_samples)\n",
    "features = features[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "print(f\"✓ Created training data:\")\n",
    "print(f\"  Samples: {features.shape[0]}\")\n",
    "print(f\"  Features: {features.shape[1]}\")\n",
    "print(f\"  Classes: {len(np.unique(labels))}\")\n",
    "print(f\"  Class distribution: {dict(zip(*np.unique(labels, return_counts=True)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training service\n",
    "trainer = ModelTrainingService({\n",
    "    'numEpochs': 50,\n",
    "    'batchSize': 32,\n",
    "    'outputDir': '../outputs/training'\n",
    "})\n",
    "\n",
    "trainer.start()\n",
    "print(\"✓ Training service initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter tuning for LSTM\n",
    "print(\"Starting hyperparameter tuning for LSTM...\")\n",
    "print(\"This may take several minutes...\\n\")\n",
    "\n",
    "lstm_tuning_results = trainer.process({\n",
    "    'action': 'tune',\n",
    "    'features': features,\n",
    "    'labels': labels,\n",
    "    'modelType': 'lstm',\n",
    "    'nTrials': 20  # Reduce for faster execution\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LSTM TUNING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for param, value in lstm_tuning_results['bestParams'].items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest Score: {lstm_tuning_results['bestScore']:.4f}\")\n",
    "print(f\"Results saved to: {lstm_tuning_results['resultsPath']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tuning results\n",
    "with open(lstm_tuning_results['resultsPath']) as f:\n",
    "    tuning_data = json.load(f)\n",
    "\n",
    "# Extract trial scores\n",
    "trial_numbers = [t['number'] for t in tuning_data['allTrials']]\n",
    "trial_scores = [t['value'] for t in tuning_data['allTrials']]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(trial_numbers, trial_scores, 'o-', alpha=0.6)\n",
    "plt.axhline(y=tuning_data['bestScore'], color='r', linestyle='--', \n",
    "            label=f\"Best: {tuning_data['bestScore']:.4f}\")\n",
    "plt.title('Hyperparameter Tuning Progress')\n",
    "plt.xlabel('Trial Number')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(trial_scores, bins=15, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=tuning_data['bestScore'], color='r', linestyle='--', \n",
    "            label=f\"Best: {tuning_data['bestScore']:.4f}\")\n",
    "plt.title('Score Distribution')\n",
    "plt.xlabel('Validation Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 5-fold cross-validation with best parameters\n",
    "print(\"Running 5-Fold Cross-Validation...\\n\")\n",
    "\n",
    "best_params = lstm_tuning_results['bestParams']\n",
    "\n",
    "cv_results = trainer.process({\n",
    "    'action': 'cross_validate',\n",
    "    'features': features,\n",
    "    'labels': labels,\n",
    "    'modelType': 'lstm',\n",
    "    'config': {\n",
    "        'inputSize': features.shape[1],\n",
    "        'numClasses': len(np.unique(labels)),\n",
    "        **{k: v for k, v in best_params.items() if k != 'learningRate'}\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "avg_metrics = cv_results['averageMetrics']\n",
    "print(f\"\\nAccuracy:  {avg_metrics['accuracy']:.4f} ± {avg_metrics['std_accuracy']:.4f}\")\n",
    "print(f\"Precision: {avg_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {avg_metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score:  {avg_metrics['f1Score']:.4f} ± {avg_metrics['std_f1Score']:.4f}\")\n",
    "\n",
    "print(f\"\\nResults saved to: {cv_results['resultsPath']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validation results\n",
    "fold_results = cv_results['foldResults']\n",
    "fold_numbers = [r['fold'] for r in fold_results]\n",
    "accuracies = [r['accuracy'] for r in fold_results]\n",
    "f1_scores = [r['f1Score'] for r in fold_results]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Accuracy per fold\n",
    "axes[0].bar(fold_numbers, accuracies, color='lightblue', edgecolor='black')\n",
    "axes[0].axhline(y=avg_metrics['accuracy'], color='r', linestyle='--', \n",
    "                label=f\"Avg: {avg_metrics['accuracy']:.4f}\")\n",
    "axes[0].fill_between([0.5, 5.5], \n",
    "                      avg_metrics['accuracy'] - avg_metrics['std_accuracy'],\n",
    "                      avg_metrics['accuracy'] + avg_metrics['std_accuracy'],\n",
    "                      alpha=0.2, color='red')\n",
    "axes[0].set_title('Accuracy per Fold')\n",
    "axes[0].set_xlabel('Fold')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_ylim([min(accuracies) - 0.05, max(accuracies) + 0.05])\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# F1-Score per fold\n",
    "axes[1].bar(fold_numbers, f1_scores, color='lightcoral', edgecolor='black')\n",
    "axes[1].axhline(y=avg_metrics['f1Score'], color='r', linestyle='--',\n",
    "                label=f\"Avg: {avg_metrics['f1Score']:.4f}\")\n",
    "axes[1].fill_between([0.5, 5.5],\n",
    "                      avg_metrics['f1Score'] - avg_metrics['std_f1Score'],\n",
    "                      avg_metrics['f1Score'] + avg_metrics['std_f1Score'],\n",
    "                      alpha=0.2, color='red')\n",
    "axes[1].set_title('F1-Score per Fold')\n",
    "axes[1].set_xlabel('Fold')\n",
    "axes[1].set_ylabel('F1-Score')\n",
    "axes[1].set_ylim([min(f1_scores) - 0.05, max(f1_scores) + 0.05])\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Final Model with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with early stopping\n",
    "print(\"Training final model with best parameters...\\n\")\n",
    "\n",
    "training_results = trainer.process({\n",
    "    'action': 'train',\n",
    "    'features': features,\n",
    "    'labels': labels,\n",
    "    'modelType': 'lstm',\n",
    "    'config': {\n",
    "        'inputSize': features.shape[1],\n",
    "        'numClasses': len(np.unique(labels)),\n",
    "        **{k: v for k, v in best_params.items() if k != 'learningRate'}\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nBest Validation Accuracy: {training_results['bestValAccuracy']:.4f}\")\n",
    "print(f\"Model saved to: {training_results['checkpointPath']}\")\n",
    "\n",
    "final_metrics = training_results['finalMetrics']\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "print(f\"  Accuracy:  {final_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {final_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {final_metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score:  {final_metrics['f1Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "history = training_results['trainingHistory']\n",
    "\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "axes[0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(epochs, history['train_acc'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "axes[1].axhline(y=training_results['bestValAccuracy'], color='g', linestyle='--',\n",
    "                label=f\"Best Val Acc: {training_results['bestValAccuracy']:.4f}\")\n",
    "axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../training_curves.png', dpi=150)\n",
    "print(\"✓ Training curves saved to: training_curves.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data\n",
    "test_features = np.random.normal(0.6, 0.12, (100, n_features))\n",
    "test_labels = np.random.randint(0, n_classes, 100)\n",
    "\n",
    "print(f\"Created test set: {test_features.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model and run inference\n",
    "from services.lstmModelService import LSTMModelService\n",
    "\n",
    "lstm_service = LSTMModelService({\n",
    "    'inputSize': features.shape[1],\n",
    "    'numClasses': len(np.unique(labels)),\n",
    "    **{k: v for k, v in best_params.items() if k != 'learningRate'},\n",
    "    'modelPath': training_results['checkpointPath']\n",
    "})\n",
    "\n",
    "lstm_service.start()\n",
    "print(\"✓ Loaded trained model\")\n",
    "\n",
    "# Run prediction\n",
    "test_results = lstm_service.process({\n",
    "    'features': test_features,\n",
    "    'metadata': {'source': 'test_set'}\n",
    "})\n",
    "\n",
    "print(f\"\\nTest Predictions (first 20): {test_results['predictions'][:20]}\")\n",
    "print(f\"Average Confidence: {np.mean(test_results['confidences']):.4f}\")\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = np.mean(np.array(test_results['predictions']) == test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "lstm_service.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison summary\n",
    "summary = pd.DataFrame({\n",
    "    'Metric': ['Best Tuning Score', 'CV Accuracy', 'CV F1-Score', \n",
    "               'Training Val Acc', 'Test Accuracy'],\n",
    "    'Value': [\n",
    "        f\"{lstm_tuning_results['bestScore']:.4f}\",\n",
    "        f\"{avg_metrics['accuracy']:.4f} ± {avg_metrics['std_accuracy']:.4f}\",\n",
    "        f\"{avg_metrics['f1Score']:.4f} ± {avg_metrics['std_f1Score']:.4f}\",\n",
    "        f\"{training_results['bestValAccuracy']:.4f}\",\n",
    "        f\"{test_accuracy:.4f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Complete Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive report\n",
    "complete_report = {\n",
    "    'experiment': {\n",
    "        'date': pd.Timestamp.now().isoformat(),\n",
    "        'dataset': {\n",
    "            'samples': int(n_samples),\n",
    "            'features': int(n_features),\n",
    "            'classes': int(n_classes)\n",
    "        }\n",
    "    },\n",
    "    'hyperparameter_tuning': {\n",
    "        'n_trials': len(tuning_data['allTrials']),\n",
    "        'best_score': float(lstm_tuning_results['bestScore']),\n",
    "        'best_params': best_params\n",
    "    },\n",
    "    'cross_validation': {\n",
    "        'n_folds': 5,\n",
    "        'accuracy': f\"{avg_metrics['accuracy']:.4f} ± {avg_metrics['std_accuracy']:.4f}\",\n",
    "        'f1_score': f\"{avg_metrics['f1Score']:.4f} ± {avg_metrics['std_f1Score']:.4f}\",\n",
    "        'precision': float(avg_metrics['precision']),\n",
    "        'recall': float(avg_metrics['recall'])\n",
    "    },\n",
    "    'final_training': {\n",
    "        'best_val_accuracy': float(training_results['bestValAccuracy']),\n",
    "        'model_path': training_results['checkpointPath'],\n",
    "        'epochs_trained': len(history['train_loss'])\n",
    "    },\n",
    "    'test_results': {\n",
    "        'accuracy': float(test_accuracy),\n",
    "        'test_samples': int(len(test_labels))\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save report\n",
    "report_path = '../model_training_report.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(complete_report, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Complete report saved to: {report_path}\")\n",
    "print(\"\\n\" + json.dumps(complete_report, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop training service\n",
    "trainer.stop()\n",
    "print(\"✓ Training service stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "1. ✓ Hyperparameter tuning with Optuna\n",
    "2. ✓ K-Fold cross-validation\n",
    "3. ✓ Model training with early stopping\n",
    "4. ✓ Visualizing training progress\n",
    "5. ✓ Loading and testing trained models\n",
    "6. ✓ Creating comprehensive reports\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Hyperparameter tuning improves model performance significantly\n",
    "- Cross-validation provides robust performance estimates\n",
    "- Early stopping prevents overfitting\n",
    "- Training history helps understand model behavior\n",
    "\n",
    "**Next Steps:**\n",
    "- Apply to real datasets (NSL-KDD, UNSW-NB15)\n",
    "- Tune CNN model\n",
    "- Compare LSTM vs CNN performance\n",
    "- Deploy trained models for inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
