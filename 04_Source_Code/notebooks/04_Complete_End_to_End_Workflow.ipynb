{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P22 IDS - Complete End-to-End Workflow\n",
    "\n",
    "This notebook demonstrates the complete workflow from data to deployment.\n",
    "\n",
    "## Pipeline Steps:\n",
    "1. Data preparation and preprocessing\n",
    "2. Hyperparameter tuning\n",
    "3. Model training\n",
    "4. Cross-validation\n",
    "5. Testing and evaluation\n",
    "6. Deployment-ready inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from orchestrator import ServiceOrchestrator\n",
    "from services.modelTrainingService import ModelTrainingService\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Setup complete\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1: DATA PREPARATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize orchestrator\n",
    "orchestrator = ServiceOrchestrator('../config.example.yaml')\n",
    "orchestrator.initialize()\n",
    "\n",
    "print(\"\\n✓ System initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or load training dataset\n",
    "# Option 1: Create synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples, n_features, n_classes = 2000, 41, 5\n",
    "\n",
    "# Generate dataset\n",
    "features = np.random.randn(n_samples, n_features)\n",
    "labels = np.random.randint(0, n_classes, n_samples)\n",
    "\n",
    "# Split into train/test\n",
    "split_idx = int(0.8 * n_samples)\n",
    "train_features, test_features = features[:split_idx], features[split_idx:]\n",
    "train_labels, test_labels = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "print(f\"\\n✓ Dataset created\")\n",
    "print(f\"  Training: {train_features.shape}\")\n",
    "print(f\"  Testing: {test_features.shape}\")\n",
    "print(f\"  Classes: {n_classes}\")\n",
    "print(f\"  Features: {n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 2: HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "trainer = ModelTrainingService({\n",
    "    'numEpochs': 50,\n",
    "    'batchSize': 32,\n",
    "    'outputDir': '../outputs/training'\n",
    "})\n",
    "trainer.start()\n",
    "\n",
    "print(\"\\nTuning LSTM hyperparameters...\")\n",
    "lstm_tuning = trainer.tuneHyperparameters(\n",
    "    features=train_features,\n",
    "    labels=train_labels,\n",
    "    modelType='lstm',\n",
    "    nTrials=25\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Hyperparameter tuning complete\")\n",
    "print(f\"Best score: {lstm_tuning['bestScore']:.4f}\")\n",
    "print(f\"Best parameters:\")\n",
    "for k, v in lstm_tuning['bestParams'].items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 3: CROSS-VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_params = lstm_tuning['bestParams']\n",
    "\n",
    "cv_results = trainer.crossValidate(\n",
    "    features=train_features,\n",
    "    labels=train_labels,\n",
    "    modelType='lstm',\n",
    "    modelConfig={\n",
    "        'inputSize': n_features,\n",
    "        'numClasses': n_classes,\n",
    "        **{k: v for k, v in best_params.items() if k != 'learningRate'}\n",
    "    }\n",
    ")\n",
    "\n",
    "avg = cv_results['averageMetrics']\n",
    "print(f\"\\n✓ Cross-validation complete (5 folds)\")\n",
    "print(f\"  Accuracy:  {avg['accuracy']:.4f} ± {avg['std_accuracy']:.4f}\")\n",
    "print(f\"  Precision: {avg['precision']:.4f}\")\n",
    "print(f\"  Recall:    {avg['recall']:.4f}\")\n",
    "print(f\"  F1-Score:  {avg['f1Score']:.4f} ± {avg['std_f1Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 4: FINAL MODEL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "training_results = trainer.trainModel(\n",
    "    features=train_features,\n",
    "    labels=train_labels,\n",
    "    modelType='lstm',\n",
    "    modelConfig={\n",
    "        'inputSize': n_features,\n",
    "        'numClasses': n_classes,\n",
    "        **{k: v for k, v in best_params.items() if k != 'learningRate'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Training complete\")\n",
    "print(f\"  Best val accuracy: {training_results['bestValAccuracy']:.4f}\")\n",
    "print(f\"  Model saved: {training_results['checkpointPath']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 5: TESTING AND EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load trained model\n",
    "from services.lstmModelService import LSTMModelService\n",
    "\n",
    "lstm_service = LSTMModelService({\n",
    "    'inputSize': n_features,\n",
    "    'numClasses': n_classes,\n",
    "    **{k: v for k, v in best_params.items() if k != 'learningRate'},\n",
    "    'modelPath': training_results['checkpointPath']\n",
    "})\n",
    "lstm_service.start()\n",
    "\n",
    "# Run predictions on test set\n",
    "test_results = lstm_service.process({\n",
    "    'features': test_features,\n",
    "    'metadata': {'source': 'test_set'}\n",
    "})\n",
    "\n",
    "predictions = np.array(test_results['predictions'])\n",
    "confidences = np.array(test_results['confidences'])\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "test_acc = accuracy_score(test_labels, predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    test_labels, predictions, average='weighted', zero_division=0\n",
    ")\n",
    "conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "print(f\"\\n✓ Test evaluation complete\")\n",
    "print(f\"  Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "print(f\"  Avg Confidence: {confidences.mean():.4f}\")\n",
    "\n",
    "lstm_service.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=range(n_classes), yticklabels=range(n_classes))\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Confusion matrix saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Production Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 6: PRODUCTION DEPLOYMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save deployment configuration\n",
    "deployment_config = {\n",
    "    'model_type': 'lstm',\n",
    "    'model_path': training_results['checkpointPath'],\n",
    "    'hyperparameters': best_params,\n",
    "    'performance': {\n",
    "        'cv_accuracy': float(avg['accuracy']),\n",
    "        'cv_std': float(avg['std_accuracy']),\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'test_f1': float(f1)\n",
    "    },\n",
    "    'dataset_info': {\n",
    "        'n_features': int(n_features),\n",
    "        'n_classes': int(n_classes),\n",
    "        'train_samples': int(len(train_labels)),\n",
    "        'test_samples': int(len(test_labels))\n",
    "    },\n",
    "    'deployment_date': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "deploy_config_path = '../deployment_config.json'\n",
    "with open(deploy_config_path, 'w') as f:\n",
    "    json.dump(deployment_config, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Deployment config saved: {deploy_config_path}\")\n",
    "print(\"\\nDeployment-ready files:\")\n",
    "print(f\"  1. Model: {training_results['checkpointPath']}\")\n",
    "print(f\"  2. Config: {deploy_config_path}\")\n",
    "print(f\"  3. CV Results: {cv_results['resultsPath']}\")\n",
    "print(f\"  4. Tuning Results: {lstm_tuning['resultsPath']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "summary_df = pd.DataFrame([\n",
    "    ['Phase 1', 'Data Preparation', f\"{n_samples} samples, {n_features} features\"],\n",
    "    ['Phase 2', 'Hyperparameter Tuning', f\"Best score: {lstm_tuning['bestScore']:.4f}\"],\n",
    "    ['Phase 3', 'Cross-Validation', f\"Accuracy: {avg['accuracy']:.4f} ± {avg['std_accuracy']:.4f}\"],\n",
    "    ['Phase 4', 'Model Training', f\"Val accuracy: {training_results['bestValAccuracy']:.4f}\"],\n",
    "    ['Phase 5', 'Testing', f\"Test accuracy: {test_acc:.4f}, F1: {f1:.4f}\"],\n",
    "    ['Phase 6', 'Deployment', 'Configuration saved']\n",
    "], columns=['Phase', 'Task', 'Result'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPLETE PIPELINE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY METRICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Tuning Best Score:     {lstm_tuning['bestScore']:.4f}\")\n",
    "print(f\"CV Accuracy:           {avg['accuracy']:.4f} ± {avg['std_accuracy']:.4f}\")\n",
    "print(f\"Training Val Accuracy: {training_results['bestValAccuracy']:.4f}\")\n",
    "print(f\"Test Accuracy:         {test_acc:.4f}\")\n",
    "print(f\"Test F1-Score:         {f1:.4f}\")\n",
    "print(f\"Avg Confidence:        {confidences.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Training history\n",
    "history = training_results['trainingHistory']\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "axes[0, 0].plot(epochs, history['train_acc'], 'b-', label='Train')\n",
    "axes[0, 0].plot(epochs, history['val_acc'], 'r-', label='Val')\n",
    "axes[0, 0].set_title('Training Progress')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# CV results\n",
    "fold_results = cv_results['foldResults']\n",
    "fold_nums = [r['fold'] for r in fold_results]\n",
    "fold_accs = [r['accuracy'] for r in fold_results]\n",
    "axes[0, 1].bar(fold_nums, fold_accs, color='steelblue', edgecolor='black')\n",
    "axes[0, 1].axhline(avg['accuracy'], color='r', linestyle='--', label='Average')\n",
    "axes[0, 1].set_title('Cross-Validation Results')\n",
    "axes[0, 1].set_xlabel('Fold')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Confidence distribution\n",
    "axes[1, 0].hist(confidences, bins=20, color='lightcoral', edgecolor='black')\n",
    "axes[1, 0].axvline(confidences.mean(), color='r', linestyle='--', \n",
    "                   label=f'Mean: {confidences.mean():.3f}')\n",
    "axes[1, 0].set_title('Prediction Confidence')\n",
    "axes[1, 0].set_xlabel('Confidence')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Performance comparison\n",
    "metrics = ['Tuning', 'CV', 'Training', 'Test']\n",
    "scores = [\n",
    "    lstm_tuning['bestScore'],\n",
    "    avg['accuracy'],\n",
    "    training_results['bestValAccuracy'],\n",
    "    test_acc\n",
    "]\n",
    "colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c']\n",
    "axes[1, 1].bar(metrics, scores, color=colors, edgecolor='black')\n",
    "axes[1, 1].set_title('Performance Across Stages')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].set_ylim([min(scores) - 0.05, 1.0])\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../pipeline_summary.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Pipeline summary visualizations saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop all services\n",
    "trainer.stop()\n",
    "orchestrator.shutdown()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ COMPLETE END-TO-END PIPELINE FINISHED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nAll outputs saved to: ../outputs/\")\n",
    "print(\"Deployment configuration: ../deployment_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Completed Pipeline:\n",
    "1. ✅ **Data Preparation** - Created/loaded dataset\n",
    "2. ✅ **Hyperparameter Tuning** - Found optimal parameters\n",
    "3. ✅ **Cross-Validation** - Validated robustness\n",
    "4. ✅ **Model Training** - Trained final model\n",
    "5. ✅ **Testing** - Evaluated on holdout set\n",
    "6. ✅ **Deployment** - Saved production-ready artifacts\n",
    "\n",
    "### Output Files:\n",
    "- Trained model checkpoint\n",
    "- Deployment configuration\n",
    "- Training history\n",
    "- CV results\n",
    "- Tuning results\n",
    "- Visualizations\n",
    "\n",
    "### Next Steps:\n",
    "- Deploy model using saved checkpoint\n",
    "- Monitor performance in production\n",
    "- Retrain periodically with new data\n",
    "- Compare with CNN model\n",
    "- Implement ensemble for best results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
